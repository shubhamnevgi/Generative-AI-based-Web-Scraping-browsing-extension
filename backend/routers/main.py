from fastapi import APIRouter, HTTPException
from backend.models import ScrapeRequest, ScrapeResponse
from backend.services.scrape import scrape_website, extract_body_content, clean_body_content, split_dom_content
from backend.services.parse import parse_with_groq
import pandas as pd
import io
import xml.etree.ElementTree as ET
import xml.sax.saxutils as saxutils
import logging
import base64

router = APIRouter()
logger = logging.getLogger(__name__)

# @router.post("/scrape_and_parse/", response_model=ScrapeResponse)
# async def scrape_and_parse(request: ScrapeRequest):
#     try:
#         logger.info(f"Received request: {request}")

#         # Validate output format
#         if request.output_format not in ["csv", "json", "excel", "xml"]:
#             raise HTTPException(status_code=400, detail="Invalid output format. Allowed values: csv, json, excel, xml.")

#         # Scrape the website
#         dom_content = scrape_website(request.url)
#         if not dom_content:
#             raise HTTPException(status_code=400, detail="Failed to scrape the website.")

#         # Extract and clean the body content
#         body_content = extract_body_content(dom_content)
#         cleaned_content = clean_body_content(body_content)

#         # Split the DOM content into smaller chunks
#         dom_chunks = split_dom_content(cleaned_content)

#         # Parse the content with Groq (always into CSV format)
#         parsed_csv = parse_with_groq(dom_chunks, request.parse_description)
#         if not parsed_csv:
#             raise HTTPException(status_code=400, detail="No relevant information found.")

#         # Convert CSV to DataFrame
#         try:
#             df = pd.read_csv(io.StringIO(parsed_csv))
#         except pd.errors.ParserError as e:
#             logger.error(f"Error parsing CSV data: {str(e)}")
#             raise HTTPException(status_code=400, detail="Invalid CSV data generated by the AI parser.")

#         # Convert DataFrame to the desired format
#         if request.output_format == "csv":
#             data = df.to_csv(index=False)
#         elif request.output_format == "json":
#             data = df.to_json(orient="records", indent=4)
#         elif request.output_format == "excel":
#             excel_file = io.BytesIO()
#             with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
#                 df.to_excel(writer, index=False)
#             excel_file.seek(0)
#             data = base64.b64encode(excel_file.getvalue()).decode("utf-8")  # Encode as base64 string
#         elif request.output_format == "xml":
#             root = ET.Element("data")
#             for _, row in df.iterrows():
#                 item = ET.SubElement(root, "item")
#                 for key, value in row.items():
#                     sanitized_key = key.replace(" ", "_").replace("(", "").replace(")", "")
#                     sanitized_value = saxutils.escape(str(value))
#                     ET.SubElement(item, sanitized_key).text = sanitized_value
#             data = ET.tostring(root, encoding="unicode", method="xml")
#         else:
#             raise HTTPException(status_code=400, detail="Invalid output format.")

#         return ScrapeResponse(status="success", data=data, message="Data processed successfully.")

#     except HTTPException as e:
#         raise e
#     except Exception as e:
#         logger.error(f"Error processing request: {str(e)}")
#         raise HTTPException(status_code=500, detail=str(e))



from fastapi.responses import JSONResponse

@router.post("/scrape_and_parse/", response_model=ScrapeResponse)
async def scrape_and_parse(request: ScrapeRequest):
    try:
        logger.info(f"Received request: {request}")

        # Validate output format
        if request.output_format not in ["csv", "json", "excel", "xml"]:
            raise HTTPException(status_code=400, detail="Invalid output format. Allowed values: csv, json, excel, xml.")

        # Scrape the website
        dom_content = scrape_website(request.url)
        if not dom_content:
            raise HTTPException(status_code=400, detail="Failed to scrape the website.")

        # Extract and clean the body content
        body_content = extract_body_content(dom_content)
        cleaned_content = clean_body_content(body_content)

        # Split the DOM content into smaller chunks
        dom_chunks = split_dom_content(cleaned_content)

        # Parse the content with Groq (always into CSV format)
        parsed_csv = parse_with_groq(dom_chunks, request.parse_description)
        if not parsed_csv:
            raise HTTPException(status_code=400, detail="No relevant information found.")

        # Convert CSV to DataFrame
        try:
            df = pd.read_csv(io.StringIO(parsed_csv))
        except pd.errors.ParserError as e:
            logger.error(f"Error parsing CSV data: {str(e)}")
            raise HTTPException(status_code=400, detail="Invalid CSV data generated by the AI parser.")

        # Convert DataFrame to the desired format
        if request.output_format == "csv":
            data = df.to_csv(index=False)
        elif request.output_format == "json":
            data = df.to_json(orient="records", indent=4)
        elif request.output_format == "excel":
            # Encode Excel file as base64 for download
            excel_file = io.BytesIO()
            with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
                df.to_excel(writer, index=False)
            excel_file.seek(0)
            encoded_excel = base64.b64encode(excel_file.getvalue()).decode("utf-8")

            # Generate HTML table for preview
            html_table = df.to_html(index=False, escape=False, classes="table table-bordered table-striped")

            return JSONResponse(content={
                "status": "success",
                "data": encoded_excel,
                "preview": html_table,
                "message": "Excel data generated successfully."
            })
        elif request.output_format == "xml":
            root = ET.Element("data")
            for _, row in df.iterrows():
                item = ET.SubElement(root, "item")
                for key, value in row.items():
                    sanitized_key = key.replace(" ", "_").replace("(", "").replace(")", "")
                    sanitized_value = saxutils.escape(str(value))
                    ET.SubElement(item, sanitized_key).text = sanitized_value
            data = ET.tostring(root, encoding="unicode", method="xml")
        else:
            raise HTTPException(status_code=400, detail="Invalid output format.")

        return ScrapeResponse(status="success", data=data, message="Data processed successfully.")

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
